{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/configs_imagenet/config_1_80_imagenet.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['save_path']= f\"{config['save_path']}width_{config['width_multiplier']}/sparsity_{config['pruning']['sparsity']*100}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(net):\n",
    "    return nn.Sequential(nn.Sequential(net.conv1, net.bn1, net.relu, net.maxpool),\n",
    "                         *net.layer1, *net.layer2, *net.layer3, *net.layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ImageNet dataset\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = cifar_dataloader(config[\"batch_size\"], config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corr_matrix(net0, net1, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Given two networks net0, net1 which each output a feature map of shape NxCxWxH, this will reshape both outputs to (N*W*H)xC \n",
    "    and then compute a CxC correlation matrix between the two.\n",
    "    \"\"\"\n",
    "    n = len(loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    corr_mtx_a = np.nan_to_num(corr_mtx_a)\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's. --> so this is permuting model1  --> model0 (i.e. π(net1))\n",
    "def get_layer_perm(net0, net1, loader):\n",
    "    corr_mtx = run_corr_matrix(net0, net1, loader)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_output(perm_map, conv, bn=None):\n",
    "    pre_weights = [conv.weight]\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([bn.weight, bn.bias, bn.running_mean, bn.running_var])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56.718, 1.8221810612143303), (58.089999999999996, 1.7481372061432625))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_1 = get_model(config)\n",
    "path1 = \"/scratch/rohan/test/imagenet_testing/width_1/sparsity_80.0/Model_A_Dense_sparsity_0.8_seed_11_epoch_5\"\n",
    "resnet50_2 = get_model(config)\n",
    "path2 = \"/scratch/rohan/test/imagenet_testing/width_1/sparsity_80.0/Model_B_Dense_sparsity_0.8_seed_11_epoch_5\"\n",
    "\n",
    "resnet50_1.load_state_dict(torch.load(path1))\n",
    "resnet50_2.load_state_dict(torch.load(path2))\n",
    "\n",
    "blocks0 = get_blocks(resnet50_1)\n",
    "blocks1 = get_blocks(resnet50_2)\n",
    "evaluate(resnet50_1,test_dl,config[\"device\"]), evaluate(resnet50_2,test_dl,config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the permutations such that the network is not functionally changed.\n",
    "# In particular, the same permutation must be applied to every conv output in a residual stream.\n",
    "def get_permk(k):\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    elif k > 0 and k <= 3:\n",
    "        return 3\n",
    "    elif k > 3 and k <= 7:\n",
    "        return 7\n",
    "    elif k > 7 and k <= 13:\n",
    "        return 13\n",
    "    elif k > 13 and k <= 16:\n",
    "        return 16\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_model_resnet50(model0, model1, loader, config):\n",
    "    last_kk = None\n",
    "    blocks0 = get_blocks(model0)\n",
    "    blocks1 = get_blocks(model1)\n",
    "    \n",
    "    for k in range(1, len(blocks1)):\n",
    "        block0 = blocks0[k]\n",
    "        block1 = blocks1[k]\n",
    "        subnet0 = nn.Sequential(blocks0[:k],\n",
    "                                block0.conv1, block0.bn1, block0.relu)\n",
    "        subnet1 = nn.Sequential(blocks1[:k],\n",
    "                                block1.conv1, block1.bn1, block1.relu)\n",
    "        perm_map = get_layer_perm(subnet0, subnet1, train_dl)\n",
    "        permute_output(perm_map, block1.conv1, block1.bn1)\n",
    "        permute_input(perm_map, block1.conv2)\n",
    "        \n",
    "        subnet0 = nn.Sequential(blocks0[:k],\n",
    "                                block0.conv1, block0.bn1, block0.relu,\n",
    "                                block0.conv2, block0.bn2, block0.relu)\n",
    "        subnet1 = nn.Sequential(blocks1[:k],\n",
    "                                block1.conv1, block1.bn1, block1.relu,\n",
    "                                block1.conv2, block1.bn2, block1.relu)\n",
    "        perm_map = get_layer_perm(subnet0, subnet1, loader)\n",
    "        permute_output(perm_map, block1.conv2, block1.bn2)\n",
    "        permute_input(perm_map, block1.conv3)\n",
    "    \n",
    "    for k in range(len(blocks1)):\n",
    "        kk = get_permk(k)\n",
    "        if kk != last_kk:\n",
    "            perm_map = get_layer_perm(blocks0[:kk+1], blocks1[:kk+1], loader)\n",
    "            last_kk = kk\n",
    "        \n",
    "        if k > 0:\n",
    "            permute_output(perm_map, blocks1[k].conv3, blocks1[k].bn3)\n",
    "            shortcut = blocks1[k].downsample\n",
    "            if shortcut:\n",
    "                permute_output(perm_map, shortcut[0], shortcut[1])\n",
    "        else:\n",
    "            permute_output(perm_map, model1.conv1, model1.bn1)\n",
    "        \n",
    "        if k+1 < len(blocks1):\n",
    "            permute_input(perm_map, blocks1[k+1].conv1)\n",
    "            shortcut = blocks1[k+1].downsample\n",
    "            if shortcut:\n",
    "                permute_input(perm_map, shortcut[0])\n",
    "        else:\n",
    "            permute_input(perm_map, model1.fc)\n",
    "    \n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5004/5004 [14:24<00:00,  5.79it/s]\n",
      " 65%|██████▍   | 3252/5004 [09:09<03:43,  7.82it/s]"
     ]
    }
   ],
   "source": [
    "permuted_model_2 = permute_model_resnet50(resnet50_1, resnet50_2, train_dl, config)\n",
    "torch.save(permuted_model_2.state_dict(), \"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/notebooks/artifacts/permuted_model_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_merged_models(model0, permuted_model_1, alpha, config):\n",
    "    model = get_model(config)\n",
    "    m1, m2 = model0.state_dict(), permuted_model_1.state_dict()\n",
    "    sd_alpha = {k: (1 - alpha) * m1[k].cuda() + alpha * m2[k].cuda()\n",
    "                for k in m1.keys()\n",
    "                if k in m2}\n",
    "    model.load_state_dict(sd_alpha, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all tracked BN stats against training data\n",
    "def reset_bn_stats(model, loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in loader:\n",
    "            output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = evaluate_merged_models(resnet50_1, permuted_model_2, 0.5, config)\n",
    "\n",
    "print('(test_acc, test_loss):')\n",
    "print('(α=0.5): %s\\t\\t<-- Merged model with neuron alignment', evaluate(model_a,test_dl,config[\"device\"]))\n",
    "reset_bn_stats(model_a, train_dl)\n",
    "print('(α=0.5): %s\\t\\t<-- Merged model with alignment + BN reset', evaluate(model_a,test_dl,config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackLayer(nn.Module):\n",
    "    def __init__(self, layer, one_d=False):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        dim = layer.conv3.out_channels\n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return (self.bn.running_mean, self.bn.running_var.sqrt())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        self.bn(x1)\n",
    "        return x1\n",
    "\n",
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer, one_d=False):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        dim = layer.conv3.out_channels\n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        return self.bn(x1)\n",
    "\n",
    "# adds TrackLayer around each block\n",
    "def make_tracked_net(net):\n",
    "    net1 = get_model(config)\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i in range(4):\n",
    "        layer = getattr(net1, 'layer%d' % (i+1))\n",
    "        for j, block in enumerate(layer):\n",
    "            layer[j] = TrackLayer(block).cuda()\n",
    "    return net1\n",
    "\n",
    "# adds ResetLayer around each block\n",
    "def make_repaired_net(net):\n",
    "    net1 = get_model(config)\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i in range(4):\n",
    "        layer = getattr(net1, 'layer%d' % (i+1))\n",
    "        for j, block in enumerate(layer):\n",
    "            layer[j] = ResetLayer(block).cuda()\n",
    "    return net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model0 = evaluate_merged_models(resnet50_1, permuted_model_2, 0, config)\n",
    "model1 = evaluate_merged_models(resnet50_1, permuted_model_2, 1, config)\n",
    "\n",
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "wrap_a = make_repaired_net(model_a)\n",
    "# Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "# around conv layers in (model0, model1, model_a).\n",
    "for track0, track1, reset_a in zip(wrap0.modules(), wrap1.modules(), wrap_a.modules()): \n",
    "    if not isinstance(track0, TrackLayer):\n",
    "        continue  \n",
    "    assert (isinstance(track0, TrackLayer)\n",
    "            and isinstance(track1, TrackLayer)\n",
    "            and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "    # get neuronal statistics of original networks\n",
    "    mu0, std0 = track0.get_stats()\n",
    "    mu1, std1 = track1.get_stats()\n",
    "    # set the goal neuronal statistics for the merged network \n",
    "    goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "    goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "    reset_a.set_stats(goal_mean, goal_std)\n",
    "\n",
    "# Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "# neuronal stats will be goal_mean and goal_std.\n",
    "reset_bn_stats(wrap_a)\n",
    "print('(α=0.5): %s\\t\\t<-- Merged models with REPAIR' % evaluate(wrap_a,test_dl,config[\"device\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_nov28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
