{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models \n",
    "from torchvision.models import ResNet50_Weights \n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from utils import get_model, yaml, cifar_dataloader, evaluate, check_hooks, calculate_overall_sparsity_from_pth, transfer_sparsity_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configs_imagenet/config_1_80_imagenet.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['save_path']= f\"{config['save_path']}width_{config['width_multiplier']}/sparsity_{config['pruning']['sparsity']*100}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50_pretrained():\n",
    "    model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    return model.cuda().eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(net):\n",
    "    return nn.Sequential(nn.Sequential(net.conv1, net.bn1, net.relu, net.maxpool),\n",
    "                         *net.layer1, *net.layer2, *net.layer3, *net.layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ImageNet dataset\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = cifar_dataloader(config[\"batch_size\"], config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(train_dl))\n",
    "# single_batch = (images, labels)\n",
    "\n",
    "# print(images.shape)\n",
    "\n",
    "# batch_size = images.shape[0]\n",
    "# print(f\"Number of data points in the batch: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is run_corr_matrix for a single batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corr_matrix_single(net0, net1, batch, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Given two networks net0, net1 which each output a feature map of shape NxCxWxH, this will reshape both outputs to (N*W*H)xC \n",
    "    and then compute a CxC correlation matrix between the two.\n",
    "    \"\"\"\n",
    "    images, labels = batch  # Unpack the single batch\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        \n",
    "        img_t = images.float().cuda()\n",
    "        out0 = net0(img_t).double()\n",
    "        out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "        out1 = net1(img_t).double()\n",
    "        out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "        # save batchwise first+second moments and outer product\n",
    "        mean0_b = out0.mean(dim=0)\n",
    "        mean1_b = out1.mean(dim=0)\n",
    "        sqmean0_b = out0.square().mean(dim=0)\n",
    "        sqmean1_b = out1.square().mean(dim=0)\n",
    "        outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "        # Initialize accumulators\n",
    "        mean0 = torch.zeros_like(mean0_b)\n",
    "        mean1 = torch.zeros_like(mean1_b)\n",
    "        sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "        sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "        outer = torch.zeros_like(outer_b)\n",
    "\n",
    "        # Accumulate statistics\n",
    "        mean0 += mean0_b\n",
    "        mean1 += mean1_b\n",
    "        sqmean0 += sqmean0_b\n",
    "        sqmean1 += sqmean1_b\n",
    "        outer += outer_b\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corr_matrix(net0, net1, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Given two networks net0, net1 which each output a feature map of shape NxCxWxH, this will reshape both outputs to (N*W*H)xC \n",
    "    and then compute a CxC correlation matrix between the two.\n",
    "    \"\"\"\n",
    "    n = len(loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    corr_mtx_a = np.nan_to_num(corr_mtx_a)\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's. --> so this is permuting model1  --> model0 (i.e. π(net1))\n",
    "def get_layer_perm(net0, net1, loader):\n",
    "    corr_mtx = run_corr_matrix(net0, net1, loader)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_output(perm_map, conv, bn=None):\n",
    "    pre_weights = [conv.weight]\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([bn.weight, bn.bias, bn.running_mean, bn.running_var])\n",
    "        \n",
    "    if hasattr(conv, 'weight_orig'):\n",
    "        pre_weights.append(conv.weight_orig)\n",
    "    if hasattr(conv, 'weight_mask'):\n",
    "        pre_weights.append(conv.weight_mask)\n",
    "        \n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]\n",
    "    \n",
    "    if hasattr(layer, 'weight_orig'):\n",
    "        layer.weight_orig.data = layer.weight_orig[:, perm_map]\n",
    "    if hasattr(layer, 'weight_mask'):\n",
    "        layer.weight_mask.data = layer.weight_mask[:, perm_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.146, 0.9644105411609825)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am just intializing a pretrained ResNet50, so that I can quickly obtain a one-shot mask. And then feed\n",
    "# the model_A_sparse into the permute model to see if it also permutes model_A_sparse.\n",
    "\n",
    "resnet50_pretrained = resnet50_pretrained()\n",
    "blocks = get_blocks(resnet50_pretrained)\n",
    "evaluate(resnet50_pretrained, test_dl, config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "from pruning import prune_model\n",
    "\n",
    "results = {}\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "model_A_sparse = copy.deepcopy(resnet50_pretrained)\n",
    "check_hooks(model_A_sparse) ## error since dense\n",
    "\n",
    "prune_model(\n",
    "    config=config,\n",
    "    model=model_A_sparse,\n",
    "    target_sparsity=config['pruning']['sparsity'],\n",
    "    optimizer_config=config['optimizer'],\n",
    "    prune_epochs=config['pruning']['prune_epochs'],\n",
    "    initial_lr=config['pruning']['prune_lr'],\n",
    "    batch_size=config['batch_size'],\n",
    "    device=device,  \n",
    "    initial_prune_perc=0.80,\n",
    "    train_epochs_per_prune=config['pruning']['train_epochs_per_prune'],\n",
    ")\n",
    "\n",
    "original_model_A_sparse = copy.deepcopy(model_A_sparse)\n",
    "\n",
    "results[\"A_sparse\"] = evaluate(\n",
    "    model_A_sparse, test_dl, config[\"device\"]\n",
    ")\n",
    "print(f\"A_sparse: {results['A_sparse'][0]:.2f}%, {results['A_sparse'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_A_sparse, \"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/notebooks/artifacts/model_A_sparse.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_1 = get_model(config)\n",
    "path1 = \"/scratch/rohan/test/imagenet_testing/width_1/sparsity_80.0/Model_A_Dense_sparsity_0.8_seed_11_epoch_5\"\n",
    "resnet50_2 = get_model(config)\n",
    "path2 = \"/scratch/rohan/test/imagenet_testing/width_1/sparsity_80.0/Model_B_Dense_sparsity_0.8_seed_11_epoch_5\"\n",
    "\n",
    "resnet50_1.load_state_dict(torch.load(path1))\n",
    "resnet50_2.load_state_dict(torch.load(path2))\n",
    "\n",
    "blocks0 = get_blocks(resnet50_1)\n",
    "blocks1 = get_blocks(resnet50_2)\n",
    "evaluate(resnet50_1,test_dl,config[\"device\"]), evaluate(resnet50_2,test_dl,config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the permutations such that the network is not functionally changed.\n",
    "# In particular, the same permutation must be applied to every conv output in a residual stream.\n",
    "def get_permk(k):\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    elif k > 0 and k <= 3:\n",
    "        return 3\n",
    "    elif k > 3 and k <= 7:\n",
    "        return 7\n",
    "    elif k > 7 and k <= 13:\n",
    "        return 13\n",
    "    elif k > 13 and k <= 16:\n",
    "        return 16\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_model_resnet50(model0, model1, modelA_sparse, loader, config):\n",
    "    last_kk = None\n",
    "    blocks0 = get_blocks(model0)\n",
    "    blocks1 = get_blocks(model1)\n",
    "    blocks_sparse = get_blocks(modelA_sparse)\n",
    "    \n",
    "    for k in range(1, len(blocks0)):\n",
    "        block0 = blocks0[k]\n",
    "        block1 = blocks1[k]\n",
    "        block_sparse = blocks_sparse[k]\n",
    "        subnet0 = nn.Sequential(blocks0[:k],\n",
    "                                block0.conv1, block0.bn1, block0.relu)\n",
    "        subnet1 = nn.Sequential(blocks1[:k],\n",
    "                                block1.conv1, block1.bn1, block1.relu)\n",
    "        # perm_map = get_layer_perm(subnet0, subnet1, train_dl) --> original if you want to permute model1 to model0\n",
    "        perm_map = get_layer_perm(subnet1, subnet0, loader)\n",
    "        permute_output(perm_map, block0.conv1, block0.bn1)\n",
    "        permute_input(perm_map, block0.conv2)\n",
    "        permute_output(perm_map, block_sparse.conv1,block_sparse.bn1)\n",
    "        permute_input(perm_map, block_sparse.conv2)\n",
    "        \n",
    "        subnet0 = nn.Sequential(blocks0[:k],\n",
    "                                block0.conv1, block0.bn1, block0.relu,\n",
    "                                block0.conv2, block0.bn2, block0.relu)\n",
    "        subnet1 = nn.Sequential(blocks1[:k],\n",
    "                                block1.conv1, block1.bn1, block1.relu,\n",
    "                                block1.conv2, block1.bn2, block1.relu)\n",
    "        perm_map = get_layer_perm(subnet1, subnet0, loader)\n",
    "        permute_output(perm_map, block1.conv2, block1.bn2)\n",
    "        permute_input(perm_map, block1.conv3)\n",
    "        permute_output(perm_map, block_sparse.conv2, block_sparse.bn2)\n",
    "        permute_input(perm_map, block_sparse.conv3)\n",
    "    \n",
    "    for k in range(len(blocks0)):\n",
    "        kk = get_permk(k)\n",
    "        if kk != last_kk:\n",
    "            perm_map = get_layer_perm(blocks1[:kk+1], blocks0[:kk+1], loader)\n",
    "            last_kk = kk\n",
    "        \n",
    "        if k > 0:\n",
    "            permute_output(perm_map, blocks0[k].conv3, blocks0[k].bn3)\n",
    "            shortcut = blocks0[k].downsample\n",
    "            if shortcut:\n",
    "                permute_output(perm_map, shortcut[0], shortcut[1])\n",
    "            permute_output(perm_map, blocks_sparse[k].conv3, blocks_sparse[k].bn3)\n",
    "            shortcut = blocks_sparse[k].downsample\n",
    "            if shortcut:\n",
    "                permute_output(perm_map, shortcut[0], shortcut[1])\n",
    "                \n",
    "        else:\n",
    "            permute_output(perm_map, model0.conv1, model0.bn1)\n",
    "            permute_output(perm_map, modelA_sparse.conv1, modelA_sparse.bn1) \n",
    "        \n",
    "        if k+1 < len(blocks0):\n",
    "            permute_input(perm_map, blocks0[k+1].conv1)\n",
    "            shortcut = blocks0[k+1].downsample\n",
    "            if shortcut:\n",
    "                permute_input(perm_map, shortcut[0])\n",
    "            permute_input(perm_map, blocks_sparse[k+1].conv1)\n",
    "            shortcut = blocks_sparse[k+1].downsample\n",
    "            if shortcut:\n",
    "                permute_input(perm_map, shortcut[0])\n",
    "        else:\n",
    "            permute_input(perm_map, model0.fc)\n",
    "            permute_input(perm_map, modelA_sparse.fc)\n",
    "            \n",
    "    \n",
    "    return model0, modelA_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 50\n",
    "\n",
    "batches = []\n",
    "for i, (images, labels) in enumerate(train_dl):\n",
    "    if i >= num_batches:\n",
    "        break\n",
    "    batches.append((images, labels))\n",
    "\n",
    "batch_size = batches[0][0].shape[0]\n",
    "\n",
    "total_data_points = num_batches * batch_size\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Total data points: {total_data_points}\")\n",
    "\n",
    "permuted_model_1, permuted_model_A_sparse = permute_model_resnet50(resnet50_1, resnet50_2, model_A_sparse, batches, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(train_dl))\n",
    "# single_batch = (images, labels)\n",
    "# permuted_model_1, permuted_model_A_sparse = permute_model_resnet50(resnet50_1, resnet50_2, model_A_sparse, single_batch, config)\n",
    "torch.save(permuted_model_1, \"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/notebooks/artifacts/permuted_model_1_multiple_batch.pth\")\n",
    "torch.save(permuted_model_A_sparse, \"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/notebooks/artifacts/permuted_model_A_sparse_multiple_batch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.604, 2.4261582411673603)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(permuted_model_1, test_dl, config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.566, 1.1008016059593277)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(permuted_model_A_sparse, test_dl, config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_merged_models(model0, permuted_model_1, alpha, config):\n",
    "    model = get_model(config)\n",
    "    m1, m2 = model0.state_dict(), permuted_model_1.state_dict()\n",
    "    sd_alpha = {k: (1 - alpha) * m1[k].cuda() + alpha * m2[k].cuda()\n",
    "                for k in m1.keys()\n",
    "                if k in m2}\n",
    "    model.load_state_dict(sd_alpha, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all tracked BN stats against training data\n",
    "def reset_bn_stats(model, loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in loader:\n",
    "            output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permuted_model_2 = get_model(config)\n",
    "# permuted_model_2.load_state_dict(torch.load(\"/home/rohan/code/sparse-rebasin-minimal/sparse-rebasin/notebooks/artifacts/permuted_model_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = evaluate_merged_models(permuted_model_1, resnet50_2, 0.5, config)\n",
    "\n",
    "print('(test_acc, test_loss):')\n",
    "print('(α=0.5): %s\\t\\t<-- Merged model with neuron alignment', evaluate(model_a,test_dl,config[\"device\"]))\n",
    "reset_bn_stats(model_a, train_dl)\n",
    "print('(α=0.5): %s\\t\\t<-- Merged model with alignment + BN reset', evaluate(model_a,test_dl,config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackLayer(nn.Module):\n",
    "    def __init__(self, layer, one_d=False):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        dim = layer.conv3.out_channels\n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return (self.bn.running_mean, self.bn.running_var.sqrt())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        self.bn(x1)\n",
    "        return x1\n",
    "\n",
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer, one_d=False):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        dim = layer.conv3.out_channels\n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        return self.bn(x1)\n",
    "\n",
    "# adds TrackLayer around each block\n",
    "def make_tracked_net(net):\n",
    "    net1 = get_model(config)\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i in range(4):\n",
    "        layer = getattr(net1, 'layer%d' % (i+1))\n",
    "        for j, block in enumerate(layer):\n",
    "            layer[j] = TrackLayer(block).cuda()\n",
    "    return net1\n",
    "\n",
    "# adds ResetLayer around each block\n",
    "def make_repaired_net(net):\n",
    "    net1 = get_model(config)\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i in range(4):\n",
    "        layer = getattr(net1, 'layer%d' % (i+1))\n",
    "        for j, block in enumerate(layer):\n",
    "            layer[j] = ResetLayer(block).cuda()\n",
    "    return net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_merged_model(permuted_model_0, model_1, model_a, train_dl, config, alpha=0.5):\n",
    "    # Evaluate merged models\n",
    "    model0 = evaluate_merged_models(permuted_model_0, model_1, 0, config)\n",
    "    model1 = evaluate_merged_models(permuted_model_0, model_1, 1, config)\n",
    "\n",
    "    # Calculate all neuronal statistics in the endpoint networks\n",
    "    wrap0 = make_tracked_net(model0)\n",
    "    wrap1 = make_tracked_net(model1)\n",
    "    reset_bn_stats(wrap0, train_dl)\n",
    "    reset_bn_stats(wrap1, train_dl)\n",
    "\n",
    "    wrap_a = make_repaired_net(model_a)\n",
    "    # Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "    # around conv layers in (model0, model1, model_a).\n",
    "    for track0, track1, reset_a in zip(wrap0.modules(), wrap1.modules(), wrap_a.modules()): \n",
    "        if not isinstance(track0, TrackLayer):\n",
    "            continue  \n",
    "        assert (isinstance(track0, TrackLayer)\n",
    "                and isinstance(track1, TrackLayer)\n",
    "                and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "        # Get neuronal statistics of original networks\n",
    "        mu0, std0 = track0.get_stats()\n",
    "        mu1, std1 = track1.get_stats()\n",
    "        # Set the goal neuronal statistics for the merged network \n",
    "        goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "        goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "        reset_a.set_stats(goal_mean, goal_std)\n",
    "\n",
    "    # Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "    # neuronal stats will be goal_mean and goal_std.\n",
    "    reset_bn_stats(wrap_a, train_dl)\n",
    "    \n",
    "    return wrap_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_model_a = repair_merged_model(permuted_model_1, resnet50_2, model_a, train_dl, config, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Merged Model with REPAIR\"] = evaluate(repaired_model_a, test_dl, config[\"device\"])\n",
    "print(f\"Merged Model with REPAIR: {results['Merged Model with REPAIR'][0]:.2f}%, {results['Merged Model with REPAIR'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing transfer sparsity\n",
    "\n",
    "# resnet50_pretrained_test = resnet50_pretrained()\n",
    "# evaluate(resnet50_pretrained_test, test_dl, config[\"device\"])\n",
    "print(\"Sparsity of the model before transfer sparsity: \",calculate_overall_sparsity_from_pth(resnet50_pretrained))\n",
    "\n",
    "transfer_sparsity_resnet(permuted_model_A_sparse, resnet50_pretrained)\n",
    "print(\"Sparsity of the init after transfer sparsity: \",calculate_overall_sparsity_from_pth(resnet50_pretrained))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_nov28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
